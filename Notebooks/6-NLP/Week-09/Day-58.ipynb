{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Day-58:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Remove Special Cherecters:\n",
    "1.Tokenization\n",
    "2.lowering\n",
    "3.stopwords remove\n",
    "4.stemming/lemitization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Nlp : Data Pre-processing: Tokenization, Stemming and Lemmatization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs='I am Abhishek Upadhyay, a dedicated and passionate professional with a strong background in [Your Industry/Field]. With [X] years of experience, I have developed a comprehensive skill set in [key skills/areas of expertise]. My goal is to leverage my knowledge and experience to contribute effectively to [specific goals or areas of interest].'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1=sent_tokenize(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am Abhishek Upadhyay, a dedicated and passionate professional with a strong background in [Your Industry/Field].',\n",
       " 'With [X] years of experience, I have developed a comprehensive skill set in [key skills/areas of expertise].',\n",
       " 'My goal is to leverage my knowledge and experience to contribute effectively to [specific goals or areas of interest].']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=word_tokenize(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence='''As Elara ventured deeper into the Whispering Woods, she encountered a mysterious figure. His name was Kael, a seasoned traveler and skilled swordsman with a hidden past. \n",
    "Initially reluctant to trust each other, they soon realized that their fates were intertwined. \n",
    "Together, they faced the perils of the forest, from enchanted beasts to treacherous terrain.\n",
    "Their journey led them to the ancient ruins of Elarian, a once-great city now lost to time.\n",
    "Here, they met Lyra, a sorceress with knowledge of the arcane and a deep connection to the Chrono Compass. \n",
    "With her guidance, they deciphered the cryptic clues on the map, each step bringing them closer to their goal.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Abhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=sent_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Stemming:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['as elara ventur deeper whisper wood , encount mysteri figur .', 'hi name kael , season travel skill swordsman hidden past .', 'initi reluct trust , soon realiz fate intertwin .', 'togeth , face peril forest , enchant beast treacher terrain .', 'their journey led ancient ruin elarian , once-great citi lost time .', 'here , met lyra , sorceress knowledg arcan deep connect chrono compass .', 'with guidanc , deciph cryptic clue map , step bring closer goal .']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentence)):\n",
    "    words=nltk.word_tokenize(sentence[i])\n",
    "    \n",
    "    words=[stemmer.stem(word) for word in words if word not in set(nltk.corpus.stopwords.words('english'))]\n",
    "    sentence[i]=' '.join(words)\n",
    "    \n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['elara ventur deeper whisper wood , encount mysteri figur .',\n",
       " 'hi name kael , season travel skill swordsman hidden past .',\n",
       " 'initi reluct trust , soon realiz fate intertwin .',\n",
       " 'togeth , face peril forest , enchant beast treacher terrain .',\n",
       " 'journey led ancient ruin elarian , once-great citi lost time .',\n",
       " ', met lyra , sorceress knowledg arcan deep connect chrono compass .',\n",
       " 'guidanc , deciph cryptic clue map , step bring closer goal .']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(sentence)):\n",
    "    words=nltk.word_tokenize(sentence[i])\n",
    "    words=[stemmer.stem(word) for word in words if word not in set(nltk.corpus.stopwords.words('english'))]\n",
    "    sentence[i]=' '.join(words)\n",
    "    \n",
    "(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemitizer:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence='''As Elara ventured deeper into the Whispering Woods, she encountered a mysterious figure. His name was Kael, a seasoned traveler and skilled swordsman with a hidden past. \n",
    "Initially reluctant to trust each other, they soon realized that their fates were intertwined. \n",
    "Together, they faced the perils of the forest, from enchanted beasts to treacherous terrain.\n",
    "Their journey led them to the ancient ruins of Elarian, a once-great city now lost to time.\n",
    "Here, they met Lyra, a sorceress with knowledge of the arcane and a deep connection to the Chrono Compass. \n",
    "With her guidance, they deciphered the cryptic clues on the map, each step bringing them closer to their goal.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Abhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "sentences=sent_tokenize(sentence)\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[lemmatizer.lemmatize(word) for word in words if word not in set(nltk.corpus.stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['As Elara ventured deeper Whispering Woods , encountered mysterious figure .', 'His name Kael , seasoned traveler skilled swordsman hidden past .', 'Initially reluctant trust , soon realized fate intertwined .', 'Together , faced peril forest , enchanted beast treacherous terrain .', 'Their journey led ancient ruin Elarian , once-great city lost time .', 'Here , met Lyra , sorceress knowledge arcane deep connection Chrono Compass .', 'With guidance , deciphered cryptic clue map , step bringing closer goal .']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['As Elara ventured deeper Whispering Woods , encountered mysterious figure .',\n",
       " 'His name Kael , seasoned traveler skilled swordsman hidden past .',\n",
       " 'Initially reluctant trust , soon realized fate intertwined .',\n",
       " 'Together , faced peril forest , enchanted beast treacherous terrain .',\n",
       " 'Their journey led ancient ruin Elarian , once-great city lost time .',\n",
       " 'Here , met Lyra , sorceress knowledge arcane deep connection Chrono Compass .',\n",
       " 'With guidance , deciphered cryptic clue map , step bringing closer goal .']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sentences)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bag of Words:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Bag of Words\" (BoW) concept is a fundamental approach used in natural language processing (NLP) and information retrieval. It simplifies the representation of text data for various applications like text classification, sentiment analysis, and document clustering. Here's a detailed explanation of the Bag of Words concept:\n",
    "\n",
    "### Key Features of the Bag of Words Model\n",
    "\n",
    "1. **Text Representation**:\n",
    "   - The BoW model represents a text document as an unordered collection of words, ignoring grammar and word order.\n",
    "   - Each document is transformed into a vector of word frequencies or binary values indicating the presence of words.\n",
    "\n",
    "2. **Vocabulary Creation**:\n",
    "   - The vocabulary is the set of unique words extracted from the entire corpus (a collection of text documents).\n",
    "   - Each word in the vocabulary is assigned a unique index.\n",
    "\n",
    "3. **Vectorization**:\n",
    "   - Each document is converted into a vector of fixed length, where the length is equal to the size of the vocabulary.\n",
    "   - The value at each index in the vector represents the frequency of the corresponding word in the document (or binary value if only the presence is considered).\n",
    "\n",
    "### Steps in Building a Bag of Words Model\n",
    "\n",
    "1. **Tokenization**:\n",
    "   - Split the text into individual words or tokens. This process may involve removing punctuation and converting text to lowercase.\n",
    "\n",
    "2. **Building the Vocabulary**:\n",
    "   - Collect all unique words from the corpus to create a vocabulary.\n",
    "   - Optionally, stop words (common words like \"and\", \"the\", etc.) can be removed, and stemming or lemmatization can be applied to reduce words to their base forms.\n",
    "\n",
    "3. **Vectorizing Documents**:\n",
    "   - Create a vector for each document based on the vocabulary.\n",
    "   - Fill in the vector with word frequencies or binary values.\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider a small corpus of two documents:\n",
    "- Document 1: \"The cat sat on the mat.\"\n",
    "- Document 2: \"The dog lay on the rug.\"\n",
    "\n",
    "#### Step 1: Tokenization\n",
    "- Document 1 Tokens: [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "- Document 2 Tokens: [\"the\", \"dog\", \"lay\", \"on\", \"the\", \"rug\"]\n",
    "\n",
    "#### Step 2: Building the Vocabulary\n",
    "- Vocabulary: [\"cat\", \"dog\", \"lay\", \"mat\", \"on\", \"rug\", \"sat\", \"the\"]\n",
    "\n",
    "#### Step 3: Vectorizing Documents\n",
    "- Document 1 Vector: [1, 0, 0, 1, 1, 0, 1, 2] (word counts: \"the\" appears twice, \"cat\", \"sat\", \"on\", \"mat\" each appear once)\n",
    "- Document 2 Vector: [0, 1, 1, 0, 1, 1, 0, 2] (word counts: \"the\" appears twice, \"dog\", \"lay\", \"on\", \"rug\" each appear once)\n",
    "\n",
    "### Applications\n",
    "\n",
    "1. **Text Classification**:\n",
    "   - BoW vectors are used as features for training machine learning models to classify text into categories (e.g., spam detection, sentiment analysis).\n",
    "\n",
    "2. **Information Retrieval**:\n",
    "   - Document vectors can be used to compare and retrieve similar documents based on word content.\n",
    "\n",
    "3. **Topic Modeling**:\n",
    "   - Techniques like Latent Dirichlet Allocation (LDA) use BoW representations to discover underlying topics in a collection of documents.\n",
    "\n",
    "### Advantages and Disadvantages\n",
    "\n",
    "**Advantages**:\n",
    "- Simplicity: Easy to implement and understand.\n",
    "- Versatility: Applicable to a wide range of text processing tasks.\n",
    "\n",
    "**Disadvantages**:\n",
    "- Loss of Context: Ignores word order and semantics.\n",
    "- High Dimensionality: Large vocabulary leads to high-dimensional vectors.\n",
    "- Sparsity: Most entries in the vectors are zero, leading to sparse data.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The Bag of Words model is a foundational technique in NLP that transforms text into a structured format suitable for machine learning algorithms. Despite its simplicity, it forms the basis for more advanced text representation methods like TF-IDF and word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Cleaning:-**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1='!@ He love % Nlp !'\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is sunny outside'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2='It is raining outside'\n",
    "re.sub('raining','sunny',sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   He love   Nlp  '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"[^a-zA-Z]\",\" \",sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "paragraph='''\n",
    "@In the bustling city of Paris, caf√©s exude a charm that's hard to resist.\n",
    "Imagine sipping a caf√© !au lait ‚òï under the iconic Eiffel Tower üóº. \n",
    "The aroma of freshly baked croissants ü•ê fills the air as artists sketch scenes of the Seine River üåä. \n",
    "A couple strolls hand in % hand, sharing a baguette ü•ñ while the distant sound of an accordion üé∂ plays a melodious tune. \n",
    "As the sun sets üåá, the city lights up, creating a magical ambiance that's truly unforgettable. \n",
    "Bon app√©tit! üåü\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=nltk.sent_tokenize(paragraph)\n",
    "corpus=[]\n",
    "for i in range(len(sentence)):\n",
    "    new_sent=re.sub(r'[^a-zA-Z]',' ',sentence[i]) #Data Cleaning Step\n",
    "    new_sent=new_sent.lower()\n",
    "    new_sent=new_sent.split()\n",
    "    #remove stopwords\n",
    "    new_sent=[stemmer.stem(word) for word in new_sent if word not in set(nltk.corpus.stopwords.words('english'))]\n",
    "    new_sent=' '.join(new_sent)\n",
    "    corpus.append(new_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bustl citi pari caf exud charm hard resist',\n",
       " 'imagin sip caf au lait icon eiffel tower',\n",
       " 'aroma freshli bake croissant fill air artist sketch scene sein river',\n",
       " 'coupl stroll hand hand share baguett distant sound accordion play melodi tune',\n",
       " 'sun set citi light creat magic ambianc truli unforgett',\n",
       " 'bon app tit',\n",
       " '']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform BOW:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(stop_words='english',lowercase=True,max_features=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 2,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        1, 0, 1],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Nlp TF IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paragraph='''\n",
    "@In the bustling city of Paris, caf√©s exude a charm that's hard to resist.\n",
    "Imagine sipping a caf√© !au lait ‚òï under the iconic Eiffel Tower üóº. \n",
    "The aroma of freshly baked croissants ü•ê fills the air as artists sketch scenes of the Seine River üåä. \n",
    "A couple strolls hand in % hand, sharing a baguette ü•ñ while the distant sound of an accordion üé∂ plays a melodious tune. \n",
    "As the sun sets üåá, the city lights up, creating a magical ambiance that's truly unforgettable. \n",
    "Bon app√©tit! üåü\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "lemmatizer=WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "token=sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\n@In the bustling city of Paris, caf√©s exude a charm that's hard to resist.\",\n",
       " 'Imagine sipping a caf√© !au lait ‚òï under the iconic Eiffel Tower üóº.',\n",
       " 'The aroma of freshly baked croissants ü•ê fills the air as artists sketch scenes of the Seine River üåä.',\n",
       " 'A couple strolls hand in % hand, sharing a baguette ü•ñ while the distant sound of an accordion üé∂ plays a melodious tune.',\n",
       " \"As the sun sets üåá, the city lights up, creating a magical ambiance that's truly unforgettable.\",\n",
       " 'Bon app√©tit!',\n",
       " 'üåü']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "for i in range(len(token)):\n",
    "    sent=re.sub(r'[^a-zA-Z]',' ',token[i])\n",
    "    sent=sent.lower()\n",
    "    words=sent.split()\n",
    "    # words=[ps.stem(word)for word in words if word not in set(nltk.corpus.stopwords.words('english'))]\n",
    "    words=[lemmatizer.lemmatize(word)for word in words if word not in set(nltk.corpus.stopwords.words('english'))]\n",
    "    new_sent=' '.join(words)\n",
    "    corpus.append(new_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bustling city paris caf exude charm hard resist',\n",
       " 'imagine sipping caf au lait iconic eiffel tower',\n",
       " 'aroma freshly baked croissant fill air artist sketch scene seine river',\n",
       " 'couple stroll hand hand sharing baguette distant sound accordion play melodious tune',\n",
       " 'sun set city light creating magical ambiance truly unforgettable',\n",
       " 'bon app tit',\n",
       " '']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer()\n",
    "x=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows √ó 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   ...  38  39  40  41  42  43  44  \\\n",
       "0   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "1   0   0   0   0   0   0   1   0   0   0  ...   1   0   0   0   0   0   1   \n",
       "2   0   1   0   0   1   1   0   0   1   0  ...   0   1   0   0   0   0   0   \n",
       "3   1   0   0   0   0   0   0   1   0   0  ...   0   0   1   1   0   0   0   \n",
       "4   0   0   1   0   0   0   0   0   0   0  ...   0   0   0   0   1   0   0   \n",
       "5   0   0   0   1   0   0   0   0   0   1  ...   0   0   0   0   0   1   0   \n",
       "6   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "\n",
       "   45  46  47  \n",
       "0   0   0   0  \n",
       "1   0   0   0  \n",
       "2   0   0   0  \n",
       "3   0   1   0  \n",
       "4   1   0   1  \n",
       "5   0   0   0  \n",
       "6   0   0   0  \n",
       "\n",
       "[7 rows x 48 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sexpr_tokenize\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph='''\n",
    "@In the bustling city of Paris, caf√©s exude a charm that's hard to resist.\n",
    "Imagine sipping a caf√© !au lait ‚òï under the iconic Eiffel Tower üóº. \n",
    "The aroma of freshly baked croissants ü•ê fills the air as artists sketch scenes of the Seine River üåä. \n",
    "A couple strolls hand in % hand, sharing a baguette ü•ñ while the distant sound of an accordion üé∂ plays a melodious tune. \n",
    "As the sun sets üåá, the city lights up, creating a magical ambiance that's truly unforgettable. \n",
    "Bon app√©tit! üåü\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\n@In the bustling city of Paris, caf√©s exude a charm that's hard to resist.\",\n",
       " 'Imagine sipping a caf√© !au lait ‚òï under the iconic Eiffel Tower üóº.',\n",
       " 'The aroma of freshly baked croissants ü•ê fills the air as artists sketch scenes of the Seine River üåä.',\n",
       " 'A couple strolls hand in % hand, sharing a baguette ü•ñ while the distant sound of an accordion üé∂ plays a melodious tune.',\n",
       " \"As the sun sets üåá, the city lights up, creating a magical ambiance that's truly unforgettable.\",\n",
       " 'Bon app√©tit!',\n",
       " 'üåü']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para=sent_tokenize(paragraph)\n",
    "para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preprocessing:\n",
    "corpus=[]\n",
    "for i in range(len(para)):\n",
    "    sent=re.sub(r'[^a-zA-Z]',' ',para[i])\n",
    "    sent=sent.lower()\n",
    "    words=sent.split()\n",
    "    words=[stemmer.stem(word) for word in words if word not in set(nltk.corpus.stopwords.words('english'))]\n",
    "    new_sent=' '.join(words)\n",
    "    corpus.append(new_sent)\n",
    "    # print(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bustl citi pari caf exud charm hard resist',\n",
       " 'imagin sip caf au lait icon eiffel tower',\n",
       " 'aroma freshli bake croissant fill air artist sketch scene sein river',\n",
       " 'coupl stroll hand hand share baguett distant sound accordion play melodi tune',\n",
       " 'sun set citi light creat magic ambianc truli unforgett',\n",
       " 'bon app tit',\n",
       " '']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
